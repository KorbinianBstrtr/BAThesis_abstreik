% !TEX root = ../main.tex
\label{section:experiment}
\section{Experiment}

Since it can be difficult to break down the effects utilising physical constraints has on a deep learning model, the previously introduced methods have to be applied to a simple, yet not trivial problem. By analysing the behaviour of our model, we aim to show that incorporating physical constraints contributes in the following ways:
\begin{itemize}
	\item Improve model performance,
	\item Create more realistic predictions,
	\item Decrease required amount of learning data.
\end{itemize}

\subsection{Problem formulation}
In order to show these improvements, we incorporate the knowledge about physical constraints into the problem of learning a rotation in two and three dimensions. We chose this problem, since it satisfies both the existence of physical constraints and the required simplicity to gain insights on the effects of the different methods of utilising physical constraints. Formally, a rotation maps a point and rotation angles to a target point:
\[rot: \mathbb{R}^{d} \times [- \pi, \pi] ^{d-1} \to \mathbb{R}^{d} \text{, where $d$ is the dimension of the domain space.} \]
For example, the rotation in two dimension can be described in the following way:
\[rot_{2D}: \mathbb{R}^{2} \times [- \pi, \pi]  \to \mathbb{R}^{2},\,\, rot_{2D}(x, \alpha) = R_{2D}(\alpha) \,x, \]
\[\text{where} \,\,R_{2D}(\alpha) = \begin{pmatrix} \cos(\alpha) & -\sin(\alpha) \\\sin(\alpha) & \cos(\alpha) \end{pmatrix}.\]
\indent For three dimensions, we will consecutively apply a rotation around the z-axis and the y-axis, both counterclockwise when looking towards the origin. This means that the rotation function is the following:
\[rot_{3D}: \mathbb{R}^{3} \times [- \pi, \pi]^2 \to \mathbb{R}^{3},\,\, rot_{3D}(x, \alpha) = R_{y}(\alpha_2) R_{z}(\alpha_1) \,x, \]
where $R_{z}(\alpha) = \begin{pmatrix} \cos(\alpha) & -\sin(\alpha) & 0\\\sin(\alpha) & \cos(\alpha) & 0\\ 0 & 0 & 1\end{pmatrix}$
and $R_{y}(\alpha) = \begin{pmatrix} \cos(\alpha) & 0 & -\sin(\alpha)\\ 0 & 1 & 0\\\sin(\alpha) & 0 & \cos(\alpha)\end{pmatrix}$.\\
\\
\indent Regarding physical constraints, we know that the determinant of any rotation matrix equals one and thereby preserves the norm of any point. Therefore, we have the following two physical constraints for our experiment:\\

\begin{subequations}
\begin{equation}
\det (R(\alpha)) = 1, \qquad \forall \alpha \in [-\pi, \pi], \forall R \in \{R_{2D}, R_z, R_y\}
\label{eq:constraint_det}
\end{equation}
\begin{equation}
||R(\alpha)x|| = ||x||, \qquad \forall \alpha \in [-\pi, \pi], \forall x \in \mathbb{R}^d, \forall R \in \{R_{2D}, R_z, R_y\}.
\label{eq:constraint_norm}
\end{equation}
\end{subequations}

\subsection{Training the Deep Learning Model}
\indent Using a deep learning model, we can now approximate the rotation function $R$ with the use of training data. For the remainder of this thesis, $N_{train}$ denotes the number of points included in our training dataset. The training data itself will be denoted $X_{train}$. It contains $N_{train}$ elements of the set $\mathbb{R}^{d} \times [- \pi, \pi] ^{d-1}$. The corresponding correct target points are denoted by $y_{train}$.\\
\indent The points to be rotated for the $2D$- and $3D$-Rotation are drawn uniformly from the unit circle and the unit sphere, respectively. The rotation angles are chosen uniformly from the set $[-\pi, \pi]^{d-1}$. Note that for three dimensions, this leads to a bias towards the poles, meaning that the target point is more likely to be either close to the original point or to the point located exactly on the opposite side of the sphere.\\
\indent When training a neural network to learn the rotation function, we solve the following minimisation problem:
\[\underset{\theta}\argmin \,\, \mathcal{L}(f_{\theta} (X_{train}), y_{train}), \]
where $f_\theta$ is the function represented by the model parameterised by $\theta$ and $L$ is a loss function measuring the dissimilarity between the model predictions $f_{\theta} (X_{train})$ and the true target points $y_{train}$. From this point on, the model predictions $f_{\theta} (X_{train})$ for a fixed parameter set $\theta$ will be denoted by $\hat{y}$.
A commonly used loss function is the Mean Squared Error (MSE), which is calculated in the following way:
\[\mathcal{L}_{MSE}(\hat{y}, y) = \frac{1}{N}\sum_{n = 1}^{N} \frac{1}{d} \sum_{i = 1}^{d} (\hat{y}_{n\,i} - y_{n\,i})^2, \qquad \text{ where } y, \hat{y} \in \mathbb{R}^{N \times d}\]

\indent All models will be trained using the MSE between the predictions and the target points. Minimising the MSE loss function fits well for our experiment, since it is closely related to the euclidian distance, except that distances are squared. This leads to high values for distant predictions and thus penalises higher variance of the errors of the predictions, which is desirable.\\ 

\subsection{Performance evaluation}
For the same reason as we chose the MSE Loss as the training loss, we also evaluate the performance of the introduced models and methods according to the MSE. Despite the MSE being a good measure for the distance between the predictions and the target point, it does not measure how well the predictions of the model align with the given physical constraints. In order to check how realistic the predictions are, we explicitely compare their norms and determinants.\\

\subsection{Network architectures}
\label{ssec:network_architectures}
\paragraph{Model 1:}The first model is a neural network directly approximating $f_\theta$. Thus it takes a point of dimension $d$ and angles of dimension $d-1$ as input and predicts a target point of dimension $d$. The network has three hidden layers with 16 nodes each. In each hidden layers, the activation function $\tanh$ is applied and we include bias. We do not apply dropout.\\

For the next models, we make use of the knowledge that a rotation is the result of multiplying a rotation matrix, which only depends on the angles, with the given point. Consequently, the next models will represent the function $g_\theta: [-\pi, \pi]^{d-1} \to \mathbb{R}^{d \times d}$, which in turn is used to create the final predictions in the following way:
\begin{equation}
\label{eq:rot_pred}
\hat{y} = g_\theta(\alpha) \, x,
\end{equation}
where $\alpha$ is the rotation angle and $x$ the point to be rotated.

\paragraph{Model 2:} Based on this approach, the second model predicts the rotation matrix. In particular, it maps $d-1$ input angles to a matrix of size $d \times d$. This is done using one hidden layer of size 100 including bias and applying the Sigmoid activation function. 

\paragraph{Model 3:} Similar to the previous model, we also try to solve the problem using an independent neural network for each of the $d \times d$ matrix entries. Each of these networks maps the rotation angles of size $d-1$ to a single real number, which is interpreted as a single matrix entry. For these networks, we use one hidden layer of size 50, again including bias and applying the Simoid activation function.\\

\indent In order to achieve undistorted comparisons of the models, they were designed to have between 600 and 650 parameters each.

\subsection{Hyperparameters}
For all experiments, we choose to use the Adaptive Moment Estimator "Adam" as our optimiser, since it empirically appeared to perform well in practice and is favourable to other known adaptive learning-method algorithms \cite{DBLP:journals/corr/Ruder16}. Furthermore we will use a learning rate of $5\times 10^{-5}$ and train each model using $50000$ iterations. Both of these parameters are empirically estimated, a comparison of using different learning rates on Model 3 can be found in the appendix (\ref{fig:comp_lr_m3}). For all experiments, we use a batch size of 512 and shuffle the training set at the start of each epoch.

\subsection{Physical loss}
In order to train our models to learn the physical constraints, we use the following loss functions for the training process.
\[\mathcal{L}(\theta, X_{train}, y_{train}) = \mathcal{L}_{MSE}(\hat{y}_\theta, y_{train}) + L_{PHY}(\hat{y}_\theta, g_\theta(X_{train})),\]
where $\hat{y}_\theta$ is computed according to equation \ref{eq:rot_pred}. For Model 1, $g_\theta$ is ignored and $\hat{y}_\theta$ is the output of the neural network.\\
For the determinant, we use the following function as our physical loss:
\[L_{DET}(\hat{y}, \hat{R}) = \frac{1}{N_{train}}\sum_{n = 1}^{N_{train}}(\det(\hat{R}_n) - 1)^2,\]
where $\hat{R}$ are the predicted rotation matrices computed by applying $g_\theta$ to each element of $X_{train}$.\\
In order to incorporate the norm constraint, the following physical loss function is applied:
\[L_{NORM}(\hat{y}, \hat{R}) = \frac{1}{N_{train}}\sum_{n = 1}^{N_{train}}(||\hat{y}_n|| - 1)^2. \]

\subsection{Solving methods}
In this section, we will introduce the methods we apply to incorporate the physical constraints into the models. As a baseline for comparisons, we train each model using solely the MSE Loss on the training data.

\subsubsection{Penalty Method}
The first and simplest version of the Penalty Method we apply is setting a fixed weight $\lambda > 0$ and solving a single minimisation problem, that is,

\[\underset{\theta}\argmin \,\, (\mathcal{L}_{MSE}(\hat{y}_\theta, y_{train}) + \lambda \cdot L_{PHY}(\hat{y}_\theta, g_\theta(X_{train}))).\]

In addition, we also solve a series of minimisation problems of the type above with exponentially increasing $\lambda$, but using the solution of the last iteration as a warm start for the next one. In particular, we introduce a multiplier $\mu > 1$ such that the weight of the physical loss in the i-th iteration is given by $\lambda_i = \mu^i \, \lambda_0$. Each minimisation is stopped as soon as the norm of the gradient is below a certain threshold.

\subsubsection{Augmented Lagrangian Method}
\label{exp:alm}
As we do for the Penalty Method, we also apply two different versions of the Augmented Lagrangian Method. Both versions minimise a fixed number of problems and update the weights of the linear constraint terms according to rule \ref{eq:alm_update}. However, the first one only computes a fixed number of epochs with a constant weight $\lambda$ for the physical loss for each minimisation problem.\\
\indent The second version was suggested by Bertsekas in \cite{Yurkiewicz1985ConstrainedOA}. In addition to increasing the weight of the squared constraint term, he proposes to stop the minimisation of the k-th problem as soon as the norm of the gradient is smaller than a threshold $\tau_k$ computed according to the following equation:
\[\tau_k = \min(\epsilon_k, \gamma_k ||c(x_k)||), \]
where $\{\epsilon_k\}$ and $\{\gamma_k \}$ are two sequences decreasing to 0 and $c(x_k)$ is the value of the constraint violation at the current solution. In our experiment, $c(x_k)$ is the difference of the norms / determinants of the predicted points / rotation matrices and one. Intuitively, this approach spends more ressources on minimising a problem if the predictions of the current solution already align well with the physical constraints and otherwise focuses on learning the constraints first.

\subsubsection{Physical Projection}
\label{sec:phys_proj}
As a third approach, we apply the Physical Projection. In particular, each prediction in the training process is projected to the closest feasable prediction. When applying it to the norm loss, we divide the predicted point $\hat{y}$ by its norm and thus get a prediction $\hat{y}'$ located on the unit circle or unit sphere. Formally, predictions are calculated the following way:
\begin{equation}
\hat{y}' = \frac{\hat{y}}{||\hat{y}||}.
\end{equation}

For the determinant, we only divide the matrix by the $d$-th root of the determinant if the latter is positive, otherwise we do not change the matrix. Therefore any predicted rotation matrix with a positive determinant is projected to a new matrix with a determinant of one. Formally, we apply the following function to each predicted matrix $\hat{R}_n$:
\begin{equation}
\hat{R}_n' = \begin{cases} \frac{\hat{R}_n}{\sqrt[d]{\det(\hat{R}_n)}}, \qquad \text{if} \det(\hat{R}_n) > 0 \\ \hat{R}_n, \qquad \qquad \,\,\,\,\text{else} \end{cases}, \qquad n = 1, ..., N_{train}.
\label{eq:norm_det}
\end{equation}

Since matrices with negative determinants are far away from the true rotation matrix, they should not occur often or if so, be corrected by solely training on the MSE-Loss for the prediction and the target point. 

\subsection{Statistical measurements}
Each experiment will be tested on 20 different seeds used to generate the training data, with a few exceptions due to time limitations. If not otherwise specified, we compare the performance of the models according to the mean of the MSEs of the individual runs. We prefer the mean over the median, because we are also interested in the performance of outliers that lead to poor results. The test set on which each model's performance is calculated consinsts of 4096 data points sampled in the same manner as the training dataset.\\



%When training a deep learning model to learn the rotation, we essentially solve this minimisation problem:\\
\[   \]



%Depending on the model architecture, 





\clearpage

