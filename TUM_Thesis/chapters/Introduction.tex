% !TEX root = ../main.tex
\label{section:introduction}
\section{Introduction}

%---------------------TODO---------------------\\

In recent years, deep learning models have proven to perform well in a wide range of real world applications. For example, neural networks have successfully been applied to the task of image classification \cite{NIPS2012_4824}, text translation \cite{DBLP:journals/corr/ChoMBB14, DBLP:journals/corr/WuSCLNMKCGMKSJL16} and autonomous driving \cite{DBLP:journals/corr/HuvalWTKSPARMCM15, Sallab:2017:2470-1173:70}. The main reason for their rise in popularity is their ability to learn the underlying system structure and scientific principles solely from training data.\\
\indent Previous to the breakthrough of deep learning, problems were tried to be solved using hand-crafted models. The latter were designed in a way to align with physical principles, such as gravity, the conservation of energy, or Newton's laws. Therefore, scientists desired to formulate mathematical theories and principles, often including empirically estimated parameters such as the gravitational constant or the speed of light. The approach of creating hand-crafted models to align with scientific principles leads to high precision and avoids the problem of generalisation, since the theories are assumed to be true across the whole domain space. Nevertheless, manually building models requires intense effort and domain knowledge, and their performance may suffer from flawed parameter estimates or theories. In particular, they can only be used when an analytical method to obtain a solution is known. \\
\indent In contrast, deep learning models do not rely on understanding the complex underlying system structure, but instead are able to learn the behaviour solely from previously observed data. For this reason, data-driven models can be used even when no scientific theories are known. Furthermore, they can in theory be applied universally to any problem, since they can approximate any continuous function \cite{Cybenko1989}. However, utilising deep learning models comes at the high cost of requiring large amounts of training data for them to create realistic predictions aligning with physical principles. In addition, the data has to cover the whole domain space, which is often not the case. A lack of data in one region will already lead to model predictions which violate scientific constraints, since the deep learning model does not specifically learn the physical rules and can therefore not generalise well.\\
\indent In this thesis, we combine the advantages of scientific knowledge with the power of deep learning models. We aim to show that theoretically constrained neural networks achieve higher performance and that their predictions are physically feasible. Further, the additional information about physical constraints reduces the required amount of training data. In order to show these improvements, we train a deep learning model to output predictions that satisfy all physical constraints. To do so, we apply methods for constrained optimisation. In particular, we analyse the results when using the Penalty Method and the Augmented Lagrangian Method. We further experiment with Physical Projection, a method which projects the predictions to the closest feasible point. In order to examine the true effects of these methods, we observe both the performance of the model on a test dataset and the physical feasibility of the predictions, and compare these statistics to the ones of a baseline model which is trained without the information about physical constraints.\\
\indent By applying the methods mentioned above to the problem of learning a rotation, we achieve significant performance improvements when utilising the Penalty Method and the Augmented Lagrangian Method. This is particularly true for small amounts of training data. We further show that the predictions of the resulting models satisfy physical constraints more accurately than the baseline model.\\
\indent Furthermore, we need to mention that the methods examined in this thesis are not limited to physically constrained problems. Instead, they can be applied to any problem where rules or constraints are known to be satisfied by the ground truth. For example, in autonomous driving, one can explicitly inform the model to only drive on streets.\\
\indent We begin the thesis by giving a broad overview of optimisation problems and important solving approaches in section \ref{ssec:opt_prob}. This is followed by theoretical background information about constrained optimisation problems in section \ref{ssec:copt_prob}. We continue in section \ref{ssec:copt_solv} by explaining the solving methods we apply and give information about their theoretical foundation and limitations. In section \ref{section:experiment}, we explain the framework of our experiments and the methods in detail. We propose several model architectures to solve the problem of learning a rotation, inform about technical details, and address the methods used to evaluate the trained models. The results of our experiments are visualised and discussed in section \ref{section:results}. Finally, we discuss the key findings of this thesis and give an outlook on what remains to be explored by future studies.

%, modeling fluid dynamics \cite{DBLP:journals/corr/SinghMD16, Tompson:2017:AEF:3305890.3306035} and simulating heat exchanger performance \cite{doi:10.1080/10789669.1999.10391233}. Traditionally, such tasks have either been impossible or otherwise simulated by hand-crafted models \cite{Cursi2005PhysicallyCN}. 

\clearpage

