% !TEX root = ../main.tex
% The abstract.
% Included by MAIN.TEX


%\phantomsection
%\addcontentsline{toc}{chapter}{Abstract}

\vspace*{2cm}
\begin{center}
{\Large \textbf{Abstract}}
\end{center}
\vspace{1cm}


Nowadays, deep learning models are applied in a wide range of different tasks, often working with real world data. They have replaced a variety of hand-crafted models, since they are faster, easier to create, and can be used even when theoretical knowledge is sparse. However, for the predictions of data-driven models to align with theoretical constraints, such as the laws of physics, large amounts of training data covering the whole domain space are required. This raises the question, whether we have to purely rely on data to learn the mathematical theories which scientists have found and experimentally validated over many years. \\
\indent In this thesis, we examine and experiment with different approaches to incorporate knowledge of physical constraints into deep learning models. In particular, we analyse the Penalty Method and Augmented Lagrangian Method. In addition, we inspect the results when projecting predictions according to physical constraints during training. The benefits of these approaches are validated on the task of predicting a rotation in two and three dimensions with the constraints on the determinant of the rotation matrix and the preservation of the norm. We confirm that the methods lead to higher overall performance already for small sizes of our training dataset and produce more realistic predictions in regions where no or only few training data is available.

