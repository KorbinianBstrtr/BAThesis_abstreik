% !TEX root = ../main.tex
% The abstract.
% Included by MAIN.TEX


%\phantomsection
%\addcontentsline{toc}{chapter}{Abstract}

\vspace*{2cm}
\begin{center}
{\Large \textbf{Abstract}}
\end{center}
\vspace{1cm}


Nowadays, deep learning models are applied in a wide range of different tasks, often working with real world data. They have replaced a variety of hand-crafted models for several reasons. Not only allows the structure of neural networks fast computation once a model is trained, but they also require less effort to create. In contrast to hand-crafted models, where every aspect of the system has to be implemented manually, data-driven models utilise the same algorithms for learning all kinds of problems and thus only sufficient hyperparameters have to be found. Furthermore, deep learning models can be used even when theoretical knowledge is sparse. However, for the predictions of data-driven models to align with theoretical constraints, such as the laws of physics, large amounts of training data covering the whole domain space are required. This raises the question, whether we have to purely rely on data to learn the mathematical theories which scientists have found and experimentally validated over many years. \\
\indent In this thesis, we examine and experiment with different approaches to incorporate knowledge of physical constraints into deep learning models. In particular, we analyse the Penalty Method and Augmented Lagrangian Method. In addition, we inspect the results when projecting predictions according to physical constraints during training. The benefits of these approaches are validated on the task of predicting a rotation in two and three dimensions with the constraints on the determinant of the rotation matrix and the preservation of the norm. We confirm that the methods lead to higher overall performance even for small training datasets and produce more realistic predictions in regions where no or only few training data is available.
