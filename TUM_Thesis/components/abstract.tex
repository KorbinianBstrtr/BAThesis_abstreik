% !TEX root = ../main.tex
% The abstract.
% Included by MAIN.TEX


%\phantomsection
%\addcontentsline{toc}{chapter}{Abstract}

\vspace*{2cm}
\begin{center}
{\Large \textbf{Abstract}}
\end{center}
\vspace{1cm}


Due to the rise of deep learning models in recent years, they are nowadays applied in a wide range of different tasks. Previously, problems could only be solved by hand-crafted models, which had to be designed individually for each task. Such models are based on principles that scientists and mathematicians have discovered and experimentally validated over many years. However, it does not only require much more effort to create a hand-crafted model, but there is also a high number of problems that can not be solved analytically, including many examples of differential equations. In contrast, deep learning models can be applied whenever training data is available, even when theoretical knowledge is sparse. However, the biggest limitation of data-driven models is the limited amount of available data for real world applications. As a consequence, their predictions deviate from the correct values and often do not align with known scientific principles. This raises the question if we purely have to rely on data to learn underlying mathematical theories which we know of and assume to be true.


%They have replaced a variety of hand-crafted models, since data-driven models require significantly less effort to create. In contrast to hand-crafted models, where every aspect of the system has to be implemented manually, deep learning models utilise the same algorithms for learning all kinds of problems and thus only sufficient hyperparameters have to be found. Furthermore, deep learning models can be used even when theoretical knowledge is sparse. However, for the predictions of data-driven models to align with theoretical constraints, such as the laws of physics, large amounts of training data covering the whole domain space are required. This raises the question, whether we have to purely rely on data to learn the mathematical theories which scientists have found and experimentally validated over many years. \\


\indent In this thesis, we examine and experiment with different approaches to incorporate knowledge of physical constraints into deep learning models. In particular, we analyse the Penalty Method and Augmented Lagrangian Method. In addition, we inspect the results when projecting predictions according to physical constraints during training. The benefits of these approaches are validated on the task of predicting a rotation in two and three dimensions with the constraints on the determinant of the rotation matrix and the preservation of the norm. We confirm that the methods lead to higher overall performance even for small training datasets and produce more realistic predictions in regions where no or only few training data is available.
